---
- name: Ollama Management Tool
  hosts: localhost
  connection: local
  gather_facts: true

  vars_prompt:
    - name: main_choice
      prompt: |
        ===== Ollama Management Tool =====
        1) Install/Update and Run Mistral
        2) Import Model from Hugging Face
        3) Advanced LLM Operations
        4) Exit
        ==================================
        Enter your choice (1-4)
      private: false

  pre_tasks:
    - name: Exit if chosen
      meta: end_play
      when: main_choice == "4"

  tasks:
    - name: Run Install/Update Mistral tasks
      when: main_choice == "1"
      block:
        - name: Check if Ollama is installed
          command: which ollama
          register: ollama_check
          failed_when: false
          changed_when: false

        - name: Install Ollama
          shell: curl -fsSL https://ollama.com/install.sh | sh
          args:
            executable: /bin/bash
          when: ollama_check.rc != 0
          register: ollama_install

        - name: Update Ollama
          shell: |
            if ollama -h | grep -q "update"; then
              ollama update
            else
              echo "Update command not available in this version of Ollama"
            fi
          when: ollama_check.rc == 0
          register: ollama_update
          changed_when: false
          failed_when: false

        - name: List available models
          command: ollama list
          register: model_list
          changed_when: false

        - name: Pull Mistral model
          command: ollama pull mistral
          when: "'mistral' not in model_list.stdout"
          register: model_pull

        - name: Run Mistral model
          command: ollama run mistral
          async: 0
          poll: 0
          register: ollama_run

        - name: Display info about Mistral running
          debug:
            msg: 
              - "âœ… MISTRAL IS NOW RUNNING!"
              - "=================================="
              - " "
              - "CONNECT TO"
              - "The Ollama CLI:"
              - "   ollama run mistral"
              - "DISCONNECT FROM:"
              - "   /bye or /exit"
              - "TO SHUT IT DOWN:"
              - "   ollama stop mistral"
              - " "
              - "=================================="

    - name: Import model from Hugging Face
      when: main_choice == "2"
      block:
        - name: Prompt for Hugging Face model URL
          pause:
            prompt: "Enter Hugging Face model URL (e.g., hf.co/Ansible-Model/santacoder-finetuned-alanstack-ec2)"
          register: model_url_input

        - name: Set model variables
          set_fact:
            model_url: "{{ model_url_input.user_input }}"
            model_name: "{{ model_url_input.user_input.split('/')[-1] }}"

        - name: Install Git LFS
          command: git lfs install
          changed_when: false

        - name: Clone model repository
          git:
            repo: "https://huggingface.co/{{ model_url }}"
            dest: "/tmp/{{ model_name }}"
            force: yes
          register: git_clone

        - name: Find GGUF model files
          find:
            paths: "/tmp/{{ model_name }}"
            patterns: "*.gguf"
          register: gguf_files

        - name: Check if GGUF files exist
          fail:
            msg: "No GGUF file found in the repository. Make sure the model supports GGUF format."
          when: gguf_files.matched == 0

        - name: Set GGUF file name
          set_fact:
            gguf_file: "{{ gguf_files.files[0].path | basename }}"

        - name: Create Modelfile
          copy:
            dest: "/tmp/{{ model_name }}/Modelfile"
            content: |
              FROM "./{{ gguf_file }}"
              TEMPLATE """
              <|system|> {{ '{{' }} .System {{ '}}' }} <|end|>
              <|user|> {{ '{{' }} .Prompt {{ '}}' }} <|end|>
              <|assistant|> {{ '{{' }} .Response {{ '}}' }} <|end|>
              """

        - name: Build model in Ollama
          command: ollama create "{{ model_name }}" -f Modelfile
          args:
            chdir: "/tmp/{{ model_name }}"
          register: ollama_create

        - name: Run the model
          command: ollama run "{{ model_name }}"
          async: 0
          poll: 0
          register: ollama_model_run

    - name: Advanced LLM Operations 
      when: main_choice == "3"
      block:
        - name: Prompt for advanced operation choice
          pause:
            prompt: |
              Select an option:
              1) Fine-tune an LLM with a dataset
              2) Use embeddings for retrieval (RAG)
              3) Fine-tune an LLM in Ollama
              Enter choice (1/2/3)
          register: advanced_choice_input

        - name: Fine-tune LLM with dataset
          when: advanced_choice_input.user_input == "1"
          block:
            - name: Prompt for model name
              pause:
                prompt: "Enter model name (e.g., mistral)"
              register: model_name_input

            - name: Prompt for dataset file
              pause:
                prompt: "Enter dataset file (e.g., my_dataset.json)"
              register: dataset_input

            - name: Run fine-tuning process
              command: python train.py --model "{{ model_name_input.user_input }}" --data "{{ dataset_input.user_input }}" --epochs 3
              register: finetune_output

            - name: Display fine-tuning output
              debug:
                var: finetune_output.stdout_lines

        - name: Use embeddings for RAG
          when: advanced_choice_input.user_input == "2"
          block:
            - name: Prompt for text data
              pause:
                prompt: "Enter text data for embedding"
              register: text_data_input

            - name: Install sentence-transformers
              pip:
                name: sentence-transformers
                state: present

            - name: Create embedding
              command: >
                python -c "
                from sentence_transformers import SentenceTransformer;
                model = SentenceTransformer('all-MiniLM-L6-v2');
                embedding = model.encode('{{ text_data_input.user_input }}');
                print('Embedding created:', embedding)"
              register: embedding_output

            - name: Display embedding output
              debug:
                var: embedding_output.stdout_lines

        - name: Fine-tune LLM in Ollama
          when: advanced_choice_input.user_input == "3"
          block:
            - name: Prompt for dataset file
              pause:
                prompt: "Enter dataset file (e.g., my_dataset.json)"
              register: dataset_input

            - name: Fine-tune model in Ollama
              command: ollama finetune mistral --data "{{ dataset_input.user_input }}" --output fine_tuned_model.gguf
              register: ollama_finetune

            - name: Display fine-tuning output
              debug:
                var: ollama_finetune.stdout_lines