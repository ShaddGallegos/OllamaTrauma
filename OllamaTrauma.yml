---
- name: Ollama Management Tool
  hosts: localhost
  connection: local
  gather_facts: true

  vars_prompt:
    - name: main_choice
      prompt: |
        ===== Ollama Management Tool =====
        1) Install/Update and Run Mistral
        2) Import Model from Hugging Face
        3) Advanced LLM Operations
        4) Exit
        ==================================
        Enter your choice (1-4)
      private: false

  pre_tasks:
    - name: Exit if chosen
      meta: end_play
      when: main_choice == "4"

  tasks:
    - name: Run Install/Update Mistral tasks
      when: main_choice == "1"
      block:
        - name: Check if Ollama is installed
          command: which ollama
          register: ollama_check
          failed_when: false
          changed_when: false

        - name: Install Ollama
          shell: curl -fsSL https://ollama.com/install.sh | sh
          args:
            executable: /bin/bash
          when: ollama_check.rc != 0
          register: ollama_install

        - name: Update Ollama
          command: ollama update
          when: ollama_check.rc == 0
          register: ollama_update

        - name: List available models
          command: ollama list
          register: model_list
          changed_when: false

        - name: Pull Mistral model
          command: ollama pull mistral
          when: "'mistral' not in model_list.stdout"
          register: model_pull

        - name: Run Mistral model
          command: ollama run mistral
          async: 0
          poll: 0
          register: ollama_run

        - name: Display info about Mistral running
          debug:
            msg: "Mistral is now running. Connect to it using your client."

    - name: Import model from Hugging Face
      when: main_choice == "2"
      block:
        - name: Prompt for Hugging Face model URL
          vars_prompt:
            - name: model_url
              prompt: "Enter Hugging Face model URL (e.g., hf.co/Ansible-Model/santacoder-finetuned-alanstack-ec2)"
              private: false
          register: model_url_input

        - name: Extract model name
          set_fact:
            model_name: "{{ model_url.split('/')[-1] }}"

        - name: Install Git LFS
          command: git lfs install
          changed_when: false

        - name: Clone model repository
          git:
            repo: "https://huggingface.co/{{ model_url }}"
            dest: "/tmp/{{ model_name }}"
            force: yes
          register: git_clone

        - name: Find GGUF model files
          find:
            paths: "/tmp/{{ model_name }}"
            patterns: "*.gguf"
          register: gguf_files

        - name: Check if GGUF files exist
          fail:
            msg: "No GGUF file found in the repository. Make sure the model supports GGUF format."
          when: gguf_files.matched == 0

        - name: Set GGUF file name
          set_fact:
            gguf_file: "{{ gguf_files.files[0].path | basename }}"

        - name: Create Modelfile
          copy:
            dest: "/tmp/{{ model_name }}/Modelfile"
            content: |
              FROM "./{{ gguf_file }}"
              TEMPLATE """
              <|system|> {{ '{{' }} .System {{ '}}' }} <|end|>
              <|user|> {{ '{{' }} .Prompt {{ '}}' }} <|end|>
              <|assistant|> {{ '{{' }} .Response {{ '}}' }} <|end|>
              """

        - name: Build model in Ollama
          command: ollama create "{{ model_name }}" -f Modelfile
          args:
            chdir: "/tmp/{{ model_name }}"
          register: ollama_create

        - name: Run the model
          command: ollama run "{{ model_name }}"
          async: 0
          poll: 0
          register: ollama_model_run

        - name: Display information about running model
          debug:
            msg: "{{ model_name }} is now running. Connect to it using your client."

    - name: Advanced LLM Operations 
      when: main_choice == "3"
      block:
        - name: Prompt for advanced operation choice
          vars_prompt:
            - name: advanced_choice
              prompt: |
                Select an option:
                1) Fine-tune an LLM with a dataset
                2) Use embeddings for retrieval (RAG)
                3) Fine-tune an LLM in Ollama
                Enter choice (1/2/3)
              private: false

        - name: Fine-tune LLM with dataset
          when: advanced_choice == "1"
          block:
            - name: Prompt for model name
              vars_prompt:
                - name: model_name
                  prompt: "Enter model name (e.g., mistral)"
                  private: false

            - name: Prompt for dataset file
              vars_prompt:
                - name: dataset
                  prompt: "Enter dataset file (e.g., my_dataset.json)"
                  private: false

            - name: Run fine-tuning process
              command: python train.py --model "{{ model_name }}" --data "{{ dataset }}" --epochs 3
              register: finetune_output

            - name: Display fine-tuning output
              debug:
                var: finetune_output.stdout_lines

        - name: Use embeddings for RAG
          when: advanced_choice == "2"
          block:
            - name: Prompt for text data
              vars_prompt:
                - name: text_data
                  prompt: "Enter text data for embedding"
                  private: false

            - name: Generate embedding
              pip:
                name: sentence-transformers
                state: present

            - name: Create embedding
              command: >
                python -c "
                from sentence_transformers import SentenceTransformer;
                model = SentenceTransformer('all-MiniLM-L6-v2');
                embedding = model.encode('{{ text_data }}');
                print('Embedding created:', embedding)"
              register: embedding_output

            - name: Display embedding output
              debug:
                var: embedding_output.stdout_lines

        - name: Fine-tune LLM in Ollama
          when: advanced_choice == "3"
          block:
            - name: Prompt for dataset file
              vars_prompt:
                - name: dataset
                  prompt: "Enter dataset file (e.g., my_dataset.json)"
                  private: false

            - name: Fine-tune model in Ollama
              command: ollama finetune mistral --data "{{ dataset }}" --output fine_tuned_model.gguf
              register: ollama_finetune

            - name: Display fine-tuning output
              debug:
                var: ollama_finetune.stdout_lines